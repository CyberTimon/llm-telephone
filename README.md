# Talk to ai using large language models and some magic tricks

# Important info
 - AI Inference server is *not* available here. You can use whatever llm backend e.g. vllm, tgi, exllamav2 you want. 

## To Do's:
 - Add proper readme
 - Add support for online openai api