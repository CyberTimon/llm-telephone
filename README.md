# Talk to ai using large language models and some tricks

# Important info
 - AI Inference server is *not* available here. You can use whatever llm backend e.g. vllm, tgi, exllamav2 you want.
 for example, you can use this: https://github.com/theroyallab/tabbyAPI

## To Do's:
 - Add proper readme
 - Add support for online openai api